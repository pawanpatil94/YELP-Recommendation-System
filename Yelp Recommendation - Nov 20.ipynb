{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/kumanik/Desktop/ASU_Work/ML/intermediate_data/split_sentences_business_final.json</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/kumanik/Desktop/ASU_Work/ML/intermediate_data/split_sentences_business_final.json"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.654584 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.654584 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[dict]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 1847 lines. Lines per second: 1353.18</pre>"
      ],
      "text/plain": [
       "Read 1847 lines. Lines per second: 1353.18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 11695 lines. Lines per second: 1819</pre>"
      ],
      "text/plain": [
       "Read 11695 lines. Lines per second: 1819"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 21041 lines. Lines per second: 1740.71</pre>"
      ],
      "text/plain": [
       "Read 21041 lines. Lines per second: 1740.71"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 30744 lines. Lines per second: 1781.23</pre>"
      ],
      "text/plain": [
       "Read 30744 lines. Lines per second: 1781.23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/kumanik/Desktop/ASU_Work/ML/intermediate_data/split_sentences_business_final.json</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/kumanik/Desktop/ASU_Work/ML/intermediate_data/split_sentences_business_final.json"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 36263 lines in 20.0917 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 36263 lines in 20.0917 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import graphlab as gl\n",
    "business = gl.SFrame.read_json('./../intermediate_data/split_sentences_business_final.json', orient='lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "business_tmp_1 = business[100:300]\n",
    "business_tmp_2 = business[300:500]\n",
    "business_tmp_3 = business[500:700]\n",
    "business_tmp_4 = business[700:900]\n",
    "business_tmp_5 = business[900:1000]\n",
    "\n",
    "\n",
    "business_tmp_1.export_json('100to300.json', orient='lines')\n",
    "business_tmp_2.export_json('300to500.json', orient='lines')\n",
    "business_tmp_3.export_json('500to700.json', orient='lines')\n",
    "business_tmp_4.export_json('700to900.json', orient='lines')\n",
    "business_tmp_5.export_json('900to1000.json', orient='lines')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from textblob import Word\n",
    "from gensim.models import Word2Vec as wv\n",
    "model = wv.load('./../word2vecModel/10_min_model_w2vec_entire_data_punc_stop_removed')\n",
    "with open('./../word2vecModel/all_bow.json', \"r\") as file_read:\n",
    "    bag_of_words = json.load(file_read)\n",
    "\n",
    "def lemmatize(list_of_words):\n",
    "    lemmatized_words = []\n",
    "    for word in list_of_words:\n",
    "        print word\n",
    "        lemmatized_words.append(Word(word).lemmatize())\n",
    "    return lemmatized_words\n",
    "    \n",
    "food_bag_of_words = lemmatize(bag_of_words['food'])\n",
    "service_bag_of_words = lemmatize(bag_of_words['service'])\n",
    "discount_bag_of_words = lemmatize(bag_of_words['discount'])\n",
    "ambience_bag_of_words = lemmatize(bag_of_words['ambience'])\n",
    "\n",
    "def remove_unpresent_words(list_of_words):\n",
    "    for word in list_of_words:\n",
    "        if not word in model.vocab:\n",
    "            list_of_words.remove(word)\n",
    "remove_unpresent_words(food_bag_of_words)\n",
    "remove_unpresent_words(service_bag_of_words)\n",
    "remove_unpresent_words(discount_bag_of_words)\n",
    "remove_unpresent_words(ambience_bag_of_words)\n",
    "            \n",
    "\n",
    "def extract_parts_of_speech(sentence):\n",
    "    parts = {}\n",
    "    adj = gl.text_analytics.extract_parts_of_speech(sentence,chosen_pos=[gl.text_analytics.PartOfSpeech.ADJ])\n",
    "    dict_of_adj = adj[0]['ADJ']\n",
    "    parts['Adjectives'] = dict_of_adj.keys()\n",
    "    noun = gl.text_analytics.extract_parts_of_speech(sentence,chosen_pos=[gl.text_analytics.PartOfSpeech.NOUN])\n",
    "    dict_of_noun = noun[0]['NOUN']\n",
    "    parts['Nouns'] = dict_of_noun.keys()\n",
    "    adverb = gl.text_analytics.extract_parts_of_speech(sentence,chosen_pos=[gl.text_analytics.PartOfSpeech.ADV])\n",
    "    dict_of_adverb = adverb[0]['ADV']\n",
    "    parts['Adverbs'] = dict_of_adverb.keys()\n",
    "    return parts\n",
    "\n",
    "def find_similarity_with_food(list_of_words):\n",
    "    if len(list_of_words) is 0:\n",
    "        return 0\n",
    "    similarity = model.n_similarity(list_of_words, food_bag_of_words)\n",
    "    return similarity\n",
    "    \n",
    "def find_similarity_with_service(list_of_words):\n",
    "    if len(list_of_words) is 0:\n",
    "            return 0\n",
    "    return model.n_similarity(list_of_words, service_bag_of_words)\n",
    "   \n",
    "def find_similarity_with_discount(list_of_words):\n",
    "    if len(list_of_words) is 0:\n",
    "        return 0\n",
    "    return model.n_similarity(list_of_words, discount_bag_of_words)\n",
    "    \n",
    "\n",
    "def find_similarity_with_ambience(list_of_words):\n",
    "    if len(list_of_words) is 0:\n",
    "        return 0\n",
    "    return model.n_similarity(list_of_words, ambience_bag_of_words)\n",
    "\n",
    "def expand_word_list(list_of_words):\n",
    "    expanded_word_list = []  \n",
    "    if len(list_of_words) is 0:\n",
    "        return expanded_word_list\n",
    "    for word in list_of_words:\n",
    "        if word in model.vocab:\n",
    "            word2vecList = model.most_similar(word, topn=5)\n",
    "            for word_tuple in word2vecList:\n",
    "                expanded_word_list.append(word_tuple[0]) \n",
    "    return expanded_word_list\n",
    "    \n",
    "count = 1\n",
    "def classify_sentences(list_of_sentences):\n",
    "    print count \n",
    "    food_sentences = []\n",
    "    service_sentences = []\n",
    "    discount_sentences = []\n",
    "    ambience_sentences = []\n",
    "    for j in range(len(list_of_sentences)):\n",
    "        sa = gl.SArray([list_of_sentences[j]])\n",
    "        parts_of_speech = extract_parts_of_speech(sa)\n",
    "        \n",
    "        \n",
    "        \n",
    "        expanded_nouns = expand_word_list(lemmatize(parts_of_speech['Nouns']))\n",
    "        expanded_adj = expand_word_list(lemmatize(parts_of_speech['Adjectives']))\n",
    "        expanded_adverb = expand_word_list(lemmatize(parts_of_speech['Adverbs']))\n",
    "    \n",
    "        food_similarity = find_similarity_with_food(expanded_nouns)\n",
    "        service_similarity = find_similarity_with_service(expanded_nouns+expanded_adj+expanded_adverb)\n",
    "        discount_similarity = find_similarity_with_discount(expanded_nouns+expanded_adj+expanded_adverb)\n",
    "        ambience_similarity = find_similarity_with_ambience(expanded_nouns+expanded_adj+expanded_adverb)\n",
    "        \n",
    "        threshold = 0.5;\n",
    "        \n",
    "        if food_similarity >= threshold:\n",
    "            food_sentences.append(list_of_sentences[j])\n",
    "        if service_similarity >= threshold:\n",
    "            service_sentences.append(list_of_sentences[j])\n",
    "        if discount_similarity >= threshold:\n",
    "            discount_sentences.append(list_of_sentences[j])\n",
    "        if ambience_similarity >= threshold:\n",
    "            ambience_sentences.append(list_of_sentences[j])\n",
    "        \n",
    "    classified_sentences = {}\n",
    "    classified_sentences['food_sentences'] = food_sentences\n",
    "    classified_sentences['service_sentences'] = service_sentences\n",
    "    classified_sentences['discount_sentences'] = discount_sentences\n",
    "    classified_sentences['ambience_sentences'] = ambience_sentences\n",
    "    count += 1\n",
    "    return classified_sentences\n",
    "    \n",
    "business_tmp['classified_sentences'] = business_tmp['splitSentences'].apply(lambda x: classify_sentences(x))\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "business_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
